### /opt/viking/ProductClassification/main.py
import argparse
import os
from src.core import RUNNERS
import src.data.transforms.img_trans
import src.data.transforms.text_trans
import src.data.dataset
import src.models.backbones.vision
import src.models.backbones.text
import src.models.heads.fusion
import src.runners.standard
import src.runners.kfold
import src.runners.test_runner
from src.utils import load_config

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True)
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])
    parser.add_argument('--resume', type=str, default=None)
    parser.add_argument('--auto-resume', action='store_true')
    
    args = parser.parse_args()
    cfg = load_config(args.config)

    if 'workflows' in cfg:
        if args.mode not in cfg['workflows']:
            raise ValueError(f"Mode '{args.mode}' not defined in workflows")
        runner_cfg = cfg['workflows'][args.mode]
    else:
        runner_cfg = cfg['runner']

    resume_path = args.resume
    if args.mode == 'train' and args.auto_resume and resume_path is None:
        work_dir = runner_cfg.get('work_dir', './work_dirs')
        latest_path = os.path.join(work_dir, 'latest.pth')
        if os.path.exists(latest_path):
            print(f"[Auto-Resume] Found: {latest_path}")
            resume_path = latest_path

    print(f"[{args.mode.upper()}] Runner: {runner_cfg['type']}")
    runner = RUNNERS.build(runner_cfg)
    
    if args.mode == 'train':
        runner.run(cfg, resume_path=resume_path)
    else:
        runner.run(cfg)

if __name__ == '__main__':
    main()

### /opt/viking/ProductClassification/dump.py
import os

def dump_files(root_dir, extensions, output_file=None):
    collected_data = []
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            if any(filename.endswith(ext) for ext in extensions):
                file_path = os.path.join(dirpath, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    collected_data.append(f"### {file_path}\n{content}\n")
                except Exception as e:
                    collected_data.append(f"### {file_path} (读取失败: {e})\n")
    result = "\n".join(collected_data)
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result)
        print(f"已导出到 {output_file}")
    else:
        print(result)

if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    dump_files(root_dir=current_dir, extensions={".py"}, output_file="code.txt")

### /opt/viking/ProductClassification/src/models/builder.py
import torch.nn as nn
from src.core import BACKBONES, HEADS

class MultiModalNet(nn.Module):
    def __init__(self, img_cfg, text_cfg, head_cfg):
        super().__init__()
        
        self.img_enc = BACKBONES.build(img_cfg)
        self.text_enc = BACKBONES.build(text_cfg)
        
        dynamic_args = {
            'img_dim': self.img_enc.out_dim,
            'text_dim': self.text_enc.out_dim
        }
        
        self.head = HEADS.build(head_cfg, **dynamic_args)

    def forward(self, img, input_ids, attention_mask):
        i_f = self.img_enc(img)
        t_f = self.text_enc(input_ids, attention_mask)
        return self.head(i_f, t_f)

def build_model(cfg):
    return MultiModalNet(cfg['img_backbone'], cfg['text_backbone'], cfg['head'])

### /opt/viking/ProductClassification/src/models/heads/fusion.py
import torch
import torch.nn as nn
from src.core import HEADS

@HEADS.register("ConcatHead")
class ConcatHead(nn.Module):
    def __init__(self, img_dim, text_dim, num_classes, dropout=0.3):
        super().__init__()
        self.bn = nn.BatchNorm1d(img_dim + text_dim)
        self.drop = nn.Dropout(dropout)
        self.fc = nn.Linear(img_dim + text_dim, num_classes)

    def forward(self, img_feat, text_feat):
        x = torch.cat([img_feat, text_feat], dim=1)
        x = self.bn(x)
        x = self.drop(x)
        return self.fc(x)


### /opt/viking/ProductClassification/src/models/backbones/vision.py
import torch.nn as nn
import torchvision.models as models
from src.core import BACKBONES

@BACKBONES.register("ResNet50")
class ResNet50(nn.Module):
    def __init__(self, model_name="resnet50", freeze=False):
        super().__init__()
        self.backbone = models.resnet50(weights='DEFAULT')
        
        self.out_dim = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        
        if freeze:
            for p in self.backbone.parameters():
                p.requires_grad = False
                
    def forward(self, x):
        return self.backbone(x)


### /opt/viking/ProductClassification/src/models/backbones/clip_encoders.py
import torch.nn as nn
from transformers import CLIPVisionModel, CLIPTextModelWithProjection
from src.core import BACKBONES
import logging

@BACKBONES.register("CLIPVision")
class CLIPVision(nn.Module):
    def __init__(self, model_name="openai/clip-vit-base-patch32", freeze=True):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Loading CLIP Vision: {model_name}")

        self.model = CLIPVisionModel.from_pretrained(model_name)

        self.out_dim = self.model.config.hidden_size
        
        if freeze:
            self.logger.info("Freezing CLIP Vision parameters")
            for p in self.model.parameters():
                p.requires_grad = False

    def forward(self, x):
        return self.model(x).pooler_output

@BACKBONES.register("CLIPText")
class CLIPText(nn.Module):
    def __init__(self, model_name="openai/clip-vit-base-patch32", freeze=True):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"Loading CLIP Text: {model_name}")

        self.model = CLIPTextModelWithProjection.from_pretrained(model_name)
        self.out_dim = self.model.config.projection_dim
        
        if freeze:
            self.logger.info("Freezing CLIP Text parameters")
            for p in self.model.parameters():
                p.requires_grad = False

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask).text_embeds

### /opt/viking/ProductClassification/src/models/backbones/text.py
import torch.nn as nn
from transformers import AutoModel
from src.core import BACKBONES
import logging

@BACKBONES.register("BertBase")
class BertBase(nn.Module):
    def __init__(self, model_name="bert-base-uncased", freeze=True):
        super().__init__()
        logger = logging.getLogger(__name__)

        logger.info(f"Loading HF Model: {model_name}")
        self.bert = AutoModel.from_pretrained(model_name)
        self.out_dim = self.bert.config.hidden_size
        
        if freeze:
            for p in self.bert.parameters():
                p.requires_grad = False
                
    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return out.last_hidden_state[:, 0, :]


### /opt/viking/ProductClassification/src/data/dataset.py
import os
import pandas as pd
import cv2
import torch
from PIL import Image 
from torch.utils.data import Dataset
from src.core import DATASETS, TRANSFORMS
import logging
import numpy as np

@DATASETS.register("ProductDataset")
class ProductDataset(Dataset):
    def __init__(self, data_root, csv_file, img_dir, img_pipeline=None, text_pipeline=None, mode='train'):
        self.data_root = data_root
        self.img_dir = os.path.join(data_root, img_dir)
        csv_path = os.path.join(data_root, csv_file)
        
        if not os.path.exists(csv_path):
             raise FileNotFoundError(f"CSV not found: {csv_path}")
             
        self.df = pd.read_csv(csv_path)
        self.mode = mode
        
        self.img_trans = []
        if img_pipeline:
            self.img_trans = TRANSFORMS.build(img_pipeline)
            
        self.text_trans = None
        if text_pipeline:
            self.text_trans = TRANSFORMS.build(text_pipeline)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        img_id = str(row['id'])
        if not img_id.endswith('.jpg'):
            img_id += '.jpg'
            
        img_path = os.path.join(self.img_dir, img_id)
        
        if os.path.exists(img_path):
            image = cv2.imread(img_path)
            if image is None:
                image = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image = np.zeros((224, 224, 3), dtype=np.uint8)

        image = Image.fromarray(image)
        
        for t in self.img_trans:
            image = t(image)
            
        text_raw = str(row['title']) + " " + str(row['description'])
        input_ids = torch.zeros(128, dtype=torch.long)
        mask = torch.zeros(128, dtype=torch.long)
        
        if self.text_trans:
            enc = self.text_trans[0](text_raw)
            input_ids = enc['input_ids'].squeeze(0)
            mask = enc['attention_mask'].squeeze(0)
            
        data = {
            'img': image,
            'input_ids': input_ids,
            'attention_mask': mask,
            'id': row['id']
        }
        
        if self.mode != 'test' and 'categories' in row:
            data['label'] = torch.tensor(row['categories'], dtype=torch.long)
            
        return data


### /opt/viking/ProductClassification/src/data/transforms/text_trans.py
from transformers import AutoTokenizer
from src.core import TRANSFORMS
import logging

@TRANSFORMS.register("BertTokenizer")
class BertTokenizerTransform:
    def __init__(self, model_name="bert-base-uncased", max_len=128):
        logger = logging.getLogger(__name__)
        self.max_len = max_len
        
        logger.info(f"[Tokenizer] Loading: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def __call__(self, text):
        return self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

### /opt/viking/ProductClassification/src/data/transforms/img_trans.py
import torchvision.transforms as T
from src.core import TRANSFORMS

@TRANSFORMS.register("Resize")
class Resize:
    def __init__(self, size):
        self.t = T.Resize(size)
    def __call__(self, img):
        return self.t(img)

@TRANSFORMS.register("Normalize")
class Normalize:
    def __init__(self, mean, std):
        self.t = T.Normalize(mean=mean, std=std)
    def __call__(self, img):
        return self.t(img)

@TRANSFORMS.register("ToTensor")
class ToTensor:
    def __init__(self):
        self.t = T.ToTensor()
    def __call__(self, img):
        return self.t(img)


### /opt/viking/ProductClassification/src/data/transforms/clip_trans.py
cat <<EOF > src/data/transforms/clip_trans.py
from transformers import CLIPProcessor
from src.core import TRANSFORMS
import logging

@TRANSFORMS.register("CLIPImageProcessor")
class CLIPImageProcessor:
    def __init__(self, model_name="openai/clip-vit-base-patch32"):
        self.processor = CLIPProcessor.from_pretrained(model_name)
    
    def __call__(self, img):
        out = self.processor(images=img, return_tensors="pt")
        return out['pixel_values'][0]

@TRANSFORMS.register("CLIPTextTokenizer")
class CLIPTextTokenizer:
    def __init__(self, model_name="openai/clip-vit-base-patch32", max_len=77):
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.max_len = max_len
        
    def __call__(self, text):
        return self.processor(
            text=text,
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
EOF

### /opt/viking/ProductClassification/src/core/__init__.py
from .registry import DATASETS, TRANSFORMS, BACKBONES, HEADS, RUNNERS, OPTIMIZERS


### /opt/viking/ProductClassification/src/core/registry.py
class Registry:
    def __init__(self, name):
        self._name = name
        self._module_dict = {}

    def register(self, name=None):
        def _register(cls):
            key = name if name else cls.__name__
            if key in self._module_dict:
                raise KeyError(f"{key} is already registered")
            self._module_dict[key] = cls
            return cls
        return _register

    def build(self, cfg, **kwargs):
        if cfg is None: return None
        if isinstance(cfg, list): return [self.build(c, **kwargs) for c in cfg]
        if not isinstance(cfg, dict) or 'type' not in cfg:
            raise TypeError("Config must be a dict with type field")

        args = cfg.copy()
        obj_type = args.pop('type')
        cls = self._module_dict.get(obj_type)
        if cls is None:
            raise KeyError(f"{obj_type} not found in {self._name}")
        return cls(**args, **kwargs)

DATASETS = Registry("datasets")
TRANSFORMS = Registry("transforms")
BACKBONES = Registry("backbones")
HEADS = Registry("heads")
RUNNERS = Registry("runners")
OPTIMIZERS = Registry("optimizers")


### /opt/viking/ProductClassification/src/utils/__init__.py
from .checkpoint import save_checkpoint, load_checkpoint
from .config import load_config
from .logger import setup_logger


### /opt/viking/ProductClassification/src/utils/checkpoint.py
import torch
import os
import logging

def save_checkpoint(state, work_dir, filename="latest.pth", is_best=False):
    os.makedirs(work_dir, exist_ok=True)
    filepath = os.path.join(work_dir, filename)
    torch.save(state, filepath)
    if is_best:
        best_path = os.path.join(work_dir, "best_model.pth")
        torch.save(state, best_path)

def load_checkpoint(model, filename, map_location='cpu'):
    logger = logging.getLogger(__name__)
    if not os.path.isfile(filename):
        raise FileNotFoundError(f"No checkpoint found at {filename}")

    logger.info(f"Loading checkpoint from {filename}")
    checkpoint = torch.load(filename, map_location=map_location)

    state_dict = checkpoint
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']

    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v

    msg = model.load_state_dict(new_state_dict, strict=False)
    logger.info(f"Missing keys: {msg.missing_keys}")
    return checkpoint


### /opt/viking/ProductClassification/src/utils/config.py
import yaml
def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)


### /opt/viking/ProductClassification/src/utils/logger.py
import logging
import os
import sys
from torch.utils.tensorboard import SummaryWriter

def setup_logger(work_dir):
    os.makedirs(work_dir, exist_ok=True)
    log_file = os.path.join(work_dir, 'train.log')

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    if logger.hasHandlers():
        logger.handlers.clear()

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    fh = logging.FileHandler(log_file)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    tb_writer = SummaryWriter(log_dir=os.path.join(work_dir, 'tf_logs'))
    return logger, tb_writer


### /opt/viking/ProductClassification/src/runners/test_runner.py
import os
import torch
import pandas as pd
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import DataLoader
from src.core import RUNNERS, DATASETS
from src.models.builder import build_model
import logging

@RUNNERS.register("TestRunner")
class TestRunner:
    def __init__(self, work_dir, batch_size=32, output_file="submission.csv", **kwargs):
        self.work_dir = work_dir
        self.batch_size = batch_size
        self.output_file = output_file
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = logging.getLogger(__name__)

    def load_models(self, cfg):
        models = []
        ckpt_files = [
            f for f in os.listdir(self.work_dir) 
            if (f.endswith('_best.pth') or f == 'best_model.pth')
        ]
        
        if not ckpt_files:
            raise FileNotFoundError(f"No .pth files found in {self.work_dir}")

        self.logger.info(f"Found {len(ckpt_files)} models for ensemble: {ckpt_files}")

        for ckpt_file in ckpt_files:
            model = build_model(cfg['model'])
            model.to(self.device)
            model.eval()
            
            ckpt_path = os.path.join(self.work_dir, ckpt_file)
            self.logger.info(f"Loading weights from {ckpt_path}...")
            
            checkpoint = torch.load(ckpt_path, map_location=self.device)
            state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
            
            new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
            
            model.load_state_dict(new_state_dict, strict=False)
            models.append(model)
            
        return models

    def run(self, cfg, **kwargs):
        self.logger.info("Starting Test/Inference Mode...")
        
        if 'test' not in cfg['dataset']:
            raise ValueError("Config must have 'dataset.test' section")
            
        test_ds = DATASETS.build(cfg['dataset']['test'])
        test_loader = DataLoader(
            test_ds, 
            batch_size=self.batch_size, 
            shuffle=False, 
            num_workers=4
        )
        
        models = self.load_models(cfg)
        
        results = []
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                img = batch['img'].to(self.device)
                ids = batch['input_ids'].to(self.device)
                mask = batch['attention_mask'].to(self.device)
                img_ids = batch['id']
                
                avg_probs = None
                for model in models:
                    logits = model(img, ids, mask)
                    probs = F.softmax(logits, dim=1)
                    
                    if avg_probs is None:
                        avg_probs = probs
                    else:
                        avg_probs += probs
                
                avg_probs /= len(models)
                preds = torch.argmax(avg_probs, dim=1).cpu().numpy()
                
                for img_id, pred_cls in zip(img_ids, preds):
                    results.append({'id': img_id, 'categories': pred_cls})

        save_path = os.path.join(self.work_dir, self.output_file)
        df = pd.DataFrame(results)
        df['id'] = df['id'].astype(str).str.replace('.jpg', '')
        df.to_csv(save_path, index=False)
        self.logger.info(f"Submission saved to: {save_path}")

### /opt/viking/ProductClassification/src/runners/standard.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from src.core import RUNNERS, DATASETS
from src.models.builder import build_model
from src.utils import setup_logger, save_checkpoint, load_checkpoint
import os
import logging

@RUNNERS.register("StandardRunner")
class StandardRunner:
    def __init__(self, work_dir, epochs, batch_size, lr, device='cuda'):
        self.work_dir = work_dir
        self.epochs = epochs
        self.batch_size = batch_size
        self.lr = lr
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.logger, self.writer = setup_logger(work_dir)
        self.best_acc = 0.0

    def train_one_epoch(self, model, loader, optim, criterion, epoch):
        model.train()
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch} Train")
        for step, batch in enumerate(pbar):
            img = batch['img'].to(self.device)
            ids = batch['input_ids'].to(self.device)
            mask = batch['attention_mask'].to(self.device)
            label = batch['label'].to(self.device)
            
            optim.zero_grad()
            preds = model(img, ids, mask)
            loss = criterion(preds, label)
            loss.backward()
            optim.step()
            
            total_loss += loss.item()
            if step % 10 == 0:
                self.writer.add_scalar('Train/Loss', loss.item(), epoch * len(loader) + step)
                pbar.set_postfix({'loss': loss.item()})
        return total_loss / len(loader)

    def validate(self, model, loader, criterion, epoch):
        model.eval()
        correct = 0
        total = 0
        val_loss = 0
        with torch.no_grad():
            for batch in tqdm(loader, desc=f"Epoch {epoch} Val"):
                img = batch['img'].to(self.device)
                ids = batch['input_ids'].to(self.device)
                mask = batch['attention_mask'].to(self.device)
                label = batch['label'].to(self.device)
                
                preds = model(img, ids, mask)
                loss = criterion(preds, label)
                val_loss += loss.item()
                
                _, predicted = torch.max(preds.data, 1)
                total += label.size(0)
                correct += (predicted == label).sum().item()
        
        acc = correct / total
        self.writer.add_scalar('Val/Acc', acc, epoch)
        self.writer.add_scalar('Val/Loss', val_loss / len(loader), epoch)
        return acc

    def run(self, cfg, resume_path=None):
        self.logger.info(f"Using device: {self.device}")
        
        # Build Data
        train_ds = DATASETS.build(cfg['dataset']['train'])
        val_ds = DATASETS.build(cfg['dataset']['val'])
        
        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)
        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False, num_workers=4)
        
        model = build_model(cfg['model'])
        model.to(self.device)
        
        optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-4)
        criterion = nn.CrossEntropyLoss()

        start_epoch = 1

        if resume_path:
            self.logger.info(f"Resuming training from: {resume_path}")
            
            checkpoint = load_checkpoint(model, resume_path)
            
            if 'optimizer' in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer'])
                    self.logger.info("Optimizer state loaded.")
                except Exception as e:
                    self.logger.warning(f"Failed to load optimizer state: {e}")

            # 恢复 epoch 和 best_acc
            if 'epoch' in checkpoint:
                start_epoch = checkpoint['epoch'] + 1
            if 'best_acc' in checkpoint:
                self.best_acc = checkpoint['best_acc']
                
            self.logger.info(f"Resumed successfully. Starting from Epoch {start_epoch}, Best Acc: {self.best_acc:.4f}")
        
        for epoch in range(start_epoch, self.epochs + 1):
            t_loss = self.train_one_epoch(model, train_loader, optimizer, criterion, epoch)
            self.logger.info(f"Epoch {epoch} Train Loss: {t_loss:.4f}")
            
            acc = self.validate(model, val_loader, criterion, epoch)
            self.logger.info(f"Epoch {epoch} Val Acc: {acc:.4f}")
            
            is_best = acc > self.best_acc
            if is_best: self.best_acc = acc
            
            save_checkpoint(
                {
                    'epoch': epoch, 
                    'state_dict': model.state_dict(), 
                    'optimizer': optimizer.state_dict(),
                    'best_acc': self.best_acc
                },
                self.work_dir,
                is_best=is_best
            )


### /opt/viking/ProductClassification/src/runners/kfold.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import StratifiedKFold
from tqdm import tqdm
import os
import numpy as np
from src.core import RUNNERS, DATASETS
from src.models.builder import build_model
from src.utils import setup_logger, save_checkpoint

@RUNNERS.register("KFoldRunner")
class KFoldRunner:
    def __init__(self, work_dir, epochs, batch_size, lr, n_splits=5, device='cuda'):
        self.work_dir = work_dir
        self.epochs = epochs
        self.batch_size = batch_size
        self.lr = lr
        self.n_splits = n_splits
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.logger, self.writer = setup_logger(work_dir)

    def train_one_epoch(self, model, loader, optim, criterion, epoch, fold):
        model.train()
        total_loss = 0
        pbar = tqdm(loader, desc=f"[Fold {fold}] Ep {epoch} Train")
        for batch in pbar:
            img = batch['img'].to(self.device)
            ids = batch['input_ids'].to(self.device)
            mask = batch['attention_mask'].to(self.device)
            label = batch['label'].to(self.device)
            
            optim.zero_grad()
            preds = model(img, ids, mask)
            loss = criterion(preds, label)
            loss.backward()
            optim.step()
            
            total_loss += loss.item()
            pbar.set_postfix({'loss': loss.item()})
        return total_loss / len(loader)

    def validate(self, model, loader, criterion, epoch, fold):
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for batch in tqdm(loader, desc=f"[Fold {fold}] Ep {epoch} Val"):
                img = batch['img'].to(self.device)
                ids = batch['input_ids'].to(self.device)
                mask = batch['attention_mask'].to(self.device)
                label = batch['label'].to(self.device)
                
                preds = model(img, ids, mask)
                _, predicted = torch.max(preds.data, 1)
                total += label.size(0)
                correct += (predicted == label).sum().item()
        return correct / total

    def run(self, cfg, resume_path=None):
        self.logger.info(f"Starting {self.n_splits}-Fold Cross Validation...")

        full_ds = DATASETS.build(cfg['dataset']['train'])
        labels = [full_ds.df.iloc[i]['categories'] for i in range(len(full_ds))]
        
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):
            fold_id = fold + 1
            self.logger.info(f"========== Fold {fold_id} / {self.n_splits} ==========")

            train_sub = Subset(full_ds, train_idx)
            val_sub = Subset(full_ds, val_idx)
            
            train_loader = DataLoader(train_sub, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)
            val_loader = DataLoader(val_sub, batch_size=self.batch_size, shuffle=False, num_workers=4)
            
            # 3. 每个 Fold 重新初始化模型
            model = build_model(cfg['model'])
            model.to(self.device)
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-4)
            criterion = nn.CrossEntropyLoss()
            
            best_acc = 0.0
            
            for epoch in range(1, self.epochs + 1):
                t_loss = self.train_one_epoch(model, train_loader, optimizer, criterion, epoch, fold_id)
                acc = self.validate(model, val_loader, criterion, epoch, fold_id)
                
                self.logger.info(f"Fold {fold_id} Ep {epoch} - Train Loss: {t_loss:.4f}, Val Acc: {acc:.4f}")
                self.writer.add_scalar(f'Fold{fold_id}/Acc', acc, epoch)

                if acc > best_acc:
                    best_acc = acc
                    save_path = os.path.join(self.work_dir, f"fold{fold_id}_best.pth")
                    torch.save({'state_dict': model.state_dict(), 'acc': best_acc}, save_path)
                    self.logger.info(f"Saved Best Fold {fold_id} model (Acc: {best_acc:.4f})")
